{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDBFtLF8ASaq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def mkdir(nonspam_train):\n",
        "    try:\n",
        "        os.mkdir(nonspam_train)\n",
        "        print('made', nonspam_train)\n",
        "    except FileExistsError:\n",
        "        print(nonspam_train, 'exists')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrsPyU6qAwoE",
        "outputId": "6f0205f6-34f0-43d4-b60e-9602689e2bf7"
      },
      "outputs": [],
      "source": [
        "mkdir('nonspam_train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm0XoxC6BG7y",
        "outputId": "23174af7-7e15-4229-855d-abc52a9b526f"
      },
      "outputs": [],
      "source": [
        "mkdir('spam_train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xukjjR70BSGS",
        "outputId": "2afb0f40-7c0f-4f8b-a90a-135d091e7d67"
      },
      "outputs": [],
      "source": [
        "mkdir('nonspam_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8zX3IxwBWSz",
        "outputId": "d8ce0343-0562-4f70-8492-c590953bc5e6"
      },
      "outputs": [],
      "source": [
        "mkdir('spam_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyN4ZvToBtMv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "# for sorting dictionaries\n",
        "from collections import OrderedDict\n",
        "from operator import itemgetter\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctDTvfK-CKIg"
      },
      "outputs": [],
      "source": [
        "# number of features\n",
        "m = 1000\n",
        "spam_train_dir = \"/content/spam_train/\"\n",
        "ham_train_dir = \"/content/nonspam_train/\"\n",
        "\n",
        "feature_dictionary_dir = \"/content/feature_dictionary.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97OyIQ0eCWqX"
      },
      "outputs": [],
      "source": [
        "# FUNCTIONS #\n",
        "\n",
        "# defines the label of the files based on their names\n",
        "def read_labels(files):\n",
        "    labels = []\n",
        "    for file in files:\n",
        "        if \"spam\" in str(file):\n",
        "            labels.append(1)\n",
        "        elif \"ham\" in str(file):\n",
        "            labels.append(0)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def get_label_frequency(train_labels, class_label):\n",
        "    frequency = 0\n",
        "    for train_label in train_labels:\n",
        "        if train_label == class_label:\n",
        "            frequency = frequency + 1\n",
        "    return frequency\n",
        "\n",
        "\n",
        "def read_file(filename):\n",
        "    text_file = open(filename, \"r\")\n",
        "    text = text_file.read()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOa6A7D_CjIA"
      },
      "outputs": [],
      "source": [
        "# extracts tokens from the given text\n",
        "def getTokens(text):\n",
        "    text_tokens = re.findall(r\"[\\w']+\", text)\n",
        "    # remove digits, special characters and convert to lowercase\n",
        "    for k in range(len(text_tokens)):\n",
        "        text_tokens[k] = text_tokens[k].lower()\n",
        "        text_tokens[k] = text_tokens[k].replace(\"_\", \"\")\n",
        "        text_tokens[k] = re.sub(\"[0-9]+\", \"\", text_tokens[k])\n",
        "    text_tokens = set(text_tokens)  # convert list to set, in order to remove duplicate tokens\n",
        "\n",
        "    return text_tokens\n",
        "\n",
        "\n",
        "def write_tokens_to_file(tokens, filename):\n",
        "    f = open(filename, 'w')\n",
        "    for token in tokens:\n",
        "        f.write(token + '\\n')\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_8TjxuTClCf",
        "outputId": "cec58379-4313-47f4-f708-3a12caa7d135"
      },
      "outputs": [],
      "source": [
        "# MAIN #\n",
        "\n",
        "spam_train_files = sorted([f for f in listdir(spam_train_dir) if isfile(join(spam_train_dir, f))])\n",
        "ham_train_files = sorted([f for f in listdir(ham_train_dir) if isfile(join(ham_train_dir, f))])\n",
        "\n",
        "train_files = list(spam_train_files)\n",
        "train_files.extend(ham_train_files)\n",
        "\n",
        "train_labels = [1] * len(spam_train_files)\n",
        "train_labels.extend([0] * len(ham_train_files))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "no_of_train_files = len(train_files)\n",
        "\n",
        "spam_class_frequency = len(spam_train_files)  # 1 is for SPAM, 0 is for HAM\n",
        "print(\"number of SPAM train documents: \" + str(spam_class_frequency))\n",
        "ham_class_frequency = len(ham_train_files)  # 1 is for SPAM, 0 is for HAM\n",
        "print(\"number of HAM train documents: \" + str(ham_class_frequency))\n",
        "\n",
        "spam_class_probability = spam_class_frequency / (len(spam_train_files) + len(ham_train_files))\n",
        "print(\"SPAM train document probability: \" + str(spam_class_probability))\n",
        "ham_class_probability = ham_class_frequency / (len(spam_train_files) + len(ham_train_files))\n",
        "print(\"HAM train document probability: \" + str(ham_class_probability))\n",
        "\n",
        "print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkWCikz4CoK4",
        "outputId": "a20ae76e-0003-4867-ca47-9a560f4ac5ce"
      },
      "outputs": [],
      "source": [
        "# feature selection with Information Gain #\n",
        "\n",
        "# a dictionary which has as a key how many documents a feature (token) appears in\n",
        "feature_frequency = dict()\n",
        "\n",
        "# a dictionary which has as a key how many train spam documents a feature appears in\n",
        "feature_spam_frequency = dict()\n",
        "\n",
        "# a dictionary which has as a key how many train ham documents a feature appears in\n",
        "feature_ham_frequency = dict()\n",
        "\n",
        "print('Calculating the frequency of each token...')\n",
        "\n",
        "# calculate feature_frequencies dict\n",
        "for i in range(len(train_files)):\n",
        "    train_text = ''\n",
        "    if train_labels[i] == 1:  # for \"SPAM\" files\n",
        "        train_text = read_file(spam_train_dir + train_files[i])\n",
        "    elif train_labels[i] == 0:  # for \"HAM\" files\n",
        "        train_text = read_file(ham_train_dir + train_files[i])\n",
        "    candidate_features = getTokens(train_text)\n",
        "\n",
        "    for token in candidate_features:\n",
        "        if token not in stop_words:\n",
        "            if token not in feature_frequency:\n",
        "                feature_frequency[token] = 1\n",
        "            else:\n",
        "                feature_frequency[token] = feature_frequency[token] + 1\n",
        "\n",
        "            if train_labels[i] == 1:  # for \"SPAM\" files\n",
        "                if token not in feature_spam_frequency:\n",
        "                    feature_spam_frequency[token] = 1\n",
        "                    feature_ham_frequency[token] = 0\n",
        "                else:\n",
        "                    feature_spam_frequency[token] = feature_spam_frequency[token] + 1\n",
        "            elif train_labels[i] == 0:  # for \"HAM\" files\n",
        "                if token not in feature_ham_frequency:\n",
        "                    feature_ham_frequency[token] = 1\n",
        "                    feature_spam_frequency[token] = 0\n",
        "                else:\n",
        "                    feature_ham_frequency[token] = feature_ham_frequency[token] + 1\n",
        "\n",
        "# sort feature_frequency dictionary in descending order by frequency\n",
        "feature_frequency = OrderedDict(sorted(feature_frequency.items(), key=itemgetter(1), reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CBHJMxLCx-P",
        "outputId": "1023b866-617a-4f7d-b347-64d1790afb0c"
      },
      "outputs": [],
      "source": [
        "# a dictionary which has as a key the probability of a feature appearing in a document\n",
        "feature_probability = dict()\n",
        "\n",
        "# a dictionary which has as a key the probability of a feature appearing in a train spam document\n",
        "feature_spam_cond_probability = dict()\n",
        "\n",
        "# a dictionary which has as a key the probability of a feature appearing in a train ham document\n",
        "feature_ham_cond_probability = dict()\n",
        "\n",
        "# a dictionary that contains the Information gain for each token\n",
        "IG = dict()\n",
        "\n",
        "# First calculate the entropy of the dataset\n",
        "H_C = - (spam_class_probability * math.log(spam_class_probability) +\n",
        "         ham_class_probability * math.log(ham_class_probability))\n",
        "\n",
        "print('entropy of the dataset: H(C) = ' + str(H_C))\n",
        "\n",
        "# precaution to avoid division by zero and log(0)\n",
        "error = 1e-7\n",
        "\n",
        "# Calculate the information gain for each candidate feature.\n",
        "# The feature that decreases the entropy less is the most desired feature,\n",
        "# because that means that it is capable of achieving better classification.\n",
        "for (i, token) in enumerate(feature_frequency):\n",
        "    if token != \"\":  # exclude the empty string \"\"\n",
        "        feature_probability[token] = feature_frequency[token] / no_of_train_files  # P(Xi=1)\n",
        "        feature_ham_cond_probability[token] = feature_ham_frequency[token] / ham_class_frequency  # P(Xi=1|C=0)\n",
        "        feature_spam_cond_probability[token] = feature_spam_frequency[token] / spam_class_frequency  # P(Xi=1|C=1)\n",
        "\n",
        "        # bayes rule: P(C=1|Xi=1) = P(Xi=1|C=1) * P(C=1) / P(Xi=1)\n",
        "        P_C1_given_X1 = feature_spam_cond_probability[token] * spam_class_probability / \\\n",
        "                        (feature_probability[token] + error)\n",
        "        # bayes rule: P(C=0|Xi=1) = P(Xi=1|C=0) * P(C=0) / P(Xi=1)\n",
        "        P_C0_given_X1 = feature_ham_cond_probability[token] * ham_class_probability / \\\n",
        "                        (feature_probability[token] + error)\n",
        "\n",
        "        # conditional entropy: H(C|Xi=1)\n",
        "        H_C_given_X1 = - (P_C1_given_X1 * math.log(P_C1_given_X1 + error) +\n",
        "                          P_C0_given_X1 * math.log(P_C0_given_X1 + error))\n",
        "\n",
        "        # bayes rule: P(C=1|Xi=0) = P(Xi=0|C=1) * P(C=1) / P(Xi=0)\n",
        "        P_C1_given_X0 = (1 - feature_spam_cond_probability[token]) * spam_class_probability / \\\n",
        "                        (1 - feature_probability[token] + error)\n",
        "        # bayes rule: P(C=0|Xi=0) = P(Xi=0|C=0) * P(C=0) / P(Xi=0)\n",
        "        P_C0_given_X0 = (1 - feature_ham_cond_probability[token]) * ham_class_probability / \\\n",
        "                        (1 - feature_probability[token] + error)\n",
        "\n",
        "        # conditional entropy: H(C|Xi=0)\n",
        "        H_C_given_X0 = - (P_C1_given_X0 * math.log(P_C1_given_X0 + error) +\n",
        "                          P_C0_given_X0 * math.log(P_C0_given_X0 + error))\n",
        "\n",
        "        # IG(C,Xi) = IG(Xi,C) = H(C) - SUM ( P(Xi=X_train) * H(C|Xi=X_train) for every X_train)\n",
        "        IG[token] = H_C - (feature_probability[token] * H_C_given_X1 + (1 - feature_probability[token]) * H_C_given_X0)\n",
        "\n",
        "        # print('{0}: P(Xi=1): {1}, P(Xi=1|C=0): {2}, P(Xi=1|C=1): {3}'.format(\n",
        "        #     token,\n",
        "        #     feature_probability[token],\n",
        "        #     feature_ham_cond_probability[token],\n",
        "        #     feature_spam_cond_probability[token]\n",
        "        # ))\n",
        "\n",
        "# MY ALTERNATIVE IG score calculation implementation\n",
        "# Calculate the information gain for each candidate feature.\n",
        "# IG is defined as the difference between the two conditional probabilities.\n",
        "# The tokens where this difference is higher have higher Information Gain.\n",
        "\"\"\"\n",
        "feature_ham_probability = dict()\n",
        "feature_spam_probability = dict()\n",
        "for (i, token) in enumerate(feature_frequency):\n",
        "    if token != \"\":  # exclude the empty string \"\"\n",
        "        feature_probability[token] = feature_frequency[token] / no_of_train_files\n",
        "        feature_ham_probability[token] = feature_ham_frequency[token] / ham_class_frequency\n",
        "        feature_spam_probability[token] = feature_spam_frequency[token] / spam_class_frequency\n",
        "        #IG[token] = feature_probability[token] * abs(feature_ham_probability[token] - feature_spam_probability[token])\n",
        "        IG[token] = abs(feature_ham_probability[token] - feature_spam_probability[token])\n",
        "        # print('{0}: P(Xi=1): {1}, P(Xi=1|C=0): {2}, P(Xi=1|C=1): {3}'.format(\n",
        "        #     token,\n",
        "        #     feature_probability[token],\n",
        "        #     feature_ham_cond_probability[token],\n",
        "        #     feature_spam_cond_probability[token]\n",
        "        # ))\n",
        "\"\"\"\n",
        "\n",
        "# sort IG dictionary in descending order by score (the higher score the better)\n",
        "IG = OrderedDict(sorted(IG.items(), key=itemgetter(1), reverse=True))\n",
        "\n",
        "print('')\n",
        "\n",
        "feature_tokens = []\n",
        "\n",
        "# create and print the list of the feature dictionary tokens and their corresponding IG scores\n",
        "print(\"feature tokens: \")\n",
        "for (i, token) in enumerate(IG):\n",
        "    # collect the m feature tokens with the highest information gain\n",
        "    if i < m:\n",
        "        feature_tokens.append(token)\n",
        "        print(token + \", information gain score: \" + str(IG[token]))\n",
        "    else:\n",
        "        break\n",
        "print('')\n",
        "\n",
        "# write feature_tokens to file\n",
        "write_tokens_to_file(feature_tokens, feature_dictionary_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgY4SenRC4of"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "feature_dictionary_dir = \"/content/feature_dictionary.txt\"\n",
        "spam_train_dir = \"/content/spam_train/\"\n",
        "ham_train_dir = \"/content/nonspam_train/\"\n",
        "spam_test_dir = \"/content/spam_test/\"\n",
        "ham_test_dir = \"/content/nonspam_test/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA1GqvAODM-_"
      },
      "outputs": [],
      "source": [
        "# FUNCTIONS #\n",
        "\n",
        "def read_dictionary_file(filename):\n",
        "    text_file = open(filename, \"r\")\n",
        "    lines = text_file.readlines()\n",
        "    for i in range(len(lines)):\n",
        "        lines[i] = lines[i].replace(\"\\n\", \"\")\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_file(filename):\n",
        "    text_file = open(filename, \"r\")\n",
        "    text = text_file.read()\n",
        "    return text\n",
        "\n",
        "\n",
        "def calculate_token_frequencies_in_class(feature_tokens, stop_words, class_documents):\n",
        "    token_frequencies_in_class = dict()  # same size as a feature vector\n",
        "    class_distinct_words = set()\n",
        "    total_words_in_class = 0\n",
        "\n",
        "    for token in feature_tokens:\n",
        "        token_frequencies_in_class[token] = 0\n",
        "\n",
        "    # For each feature token count how many times the documents of the given class contain it.\n",
        "    for i, document in enumerate(class_documents):\n",
        "        # print('document:', document)\n",
        "        tokenized_document = word_tokenize(document)\n",
        "        # print('tokenized_document:', tokenized_document)\n",
        "        filtered_document = [w.lower() for w in tokenized_document if not w.lower() in stop_words]\n",
        "        # print('filtered_document:', filtered_document)\n",
        "        for word in filtered_document:\n",
        "            if word in feature_tokens:\n",
        "                token_frequencies_in_class[word] = token_frequencies_in_class[word] + 1\n",
        "\n",
        "        document_set = set(filtered_document)\n",
        "        class_distinct_words = class_distinct_words.union(document_set)\n",
        "\n",
        "        # number_of_class_words += len(tokenized_document)\n",
        "        total_words_in_class += len(filtered_document)\n",
        "\n",
        "    # number_of_class_words = len(class_distinct_words)\n",
        "    return token_frequencies_in_class, class_distinct_words, total_words_in_class\n",
        "\n",
        "\n",
        "def calculate_laplace_estimate_probability(\n",
        "        test_feature_vector,\n",
        "        feature_tokens,\n",
        "        class_probability,\n",
        "        token_frequencies_in_class,\n",
        "        total_words_in_class,\n",
        "        V\n",
        "):\n",
        "    # laplace_estimate_probability = 1\n",
        "    laplace_estimate_log_probability = 0\n",
        "    for i, test_feature in enumerate(test_feature_vector):\n",
        "        token = feature_tokens[i]\n",
        "        if test_feature == 1:\n",
        "            if token in token_frequencies_in_class:\n",
        "                probOfTokenBelongingToClass = (token_frequencies_in_class[token] + 1) / (total_words_in_class + V)\n",
        "            else:\n",
        "                probOfTokenBelongingToClass = (0 + 1) / (total_words_in_class + V)\n",
        "\n",
        "            # traditional way: using multiplications of probabilities\n",
        "            # laplace_estimate_probability *= probOfTokenBelongingToClass\n",
        "\n",
        "            # numerically stable way to avoid multiplications of probabilities\n",
        "            laplace_estimate_log_probability += math.log(probOfTokenBelongingToClass, 2)\n",
        "\n",
        "    # laplace_estimate_probability *= class_probability\n",
        "    laplace_estimate_log_probability += math.log(class_probability, 2)\n",
        "\n",
        "    # return laplace_estimate_probability\n",
        "    return laplace_estimate_log_probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2S6FPs0DQXv",
        "outputId": "b87f7d2e-01ff-4a3d-a382-12dc50d2db16"
      },
      "outputs": [],
      "source": [
        "# MAIN #\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    spam_train_files = sorted([f for f in listdir(spam_train_dir) if isfile(join(spam_train_dir, f))])\n",
        "    ham_train_files = sorted([f for f in listdir(ham_train_dir) if isfile(join(ham_train_dir, f))])\n",
        "    spam_test_files = sorted([f for f in listdir(spam_test_dir) if isfile(join(spam_test_dir, f))])\n",
        "    ham_test_files = sorted([f for f in listdir(ham_test_dir) if isfile(join(ham_test_dir, f))])\n",
        "\n",
        "    train_files = list(spam_train_files)\n",
        "    train_files.extend(ham_train_files)\n",
        "\n",
        "    test_files = list(spam_test_files)\n",
        "    test_files.extend(ham_test_files)\n",
        "\n",
        "    train_labels = [1] * len(spam_train_files)\n",
        "    train_labels.extend([0] * len(ham_train_files))\n",
        "\n",
        "    test_true_labels = [1] * len(spam_test_files)\n",
        "    test_true_labels.extend([0] * len(ham_test_files))\n",
        "\n",
        "    spam_class_frequency = len(spam_train_files)  # 1 is for SPAM, 0 is for HAM\n",
        "    print(\"number of SPAM train documents: \" + str(spam_class_frequency))\n",
        "    ham_class_frequency = len(ham_train_files)  # 1 is for SPAM, 0 is for HAM\n",
        "    print(\"number of HAM train documents: \" + str(ham_class_frequency))\n",
        "\n",
        "    spam_class_probability = spam_class_frequency / (len(spam_train_files) + len(ham_train_files))\n",
        "    print(\"SPAM train document probability: \" + str(spam_class_probability))\n",
        "    ham_class_probability = ham_class_frequency / (len(spam_train_files) + len(ham_train_files))\n",
        "    print(\"HAM train document probability: \" + str(ham_class_probability))\n",
        "\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57YU2a_VDS4f"
      },
      "outputs": [],
      "source": [
        "feature_tokens = read_dictionary_file(feature_dictionary_dir)\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLxXrQvrDy1v",
        "outputId": "2738d982-4e7c-48cb-fbce-1c4be5ace7ec"
      },
      "outputs": [],
      "source": [
        "print(\"Reading TRAIN files...\")\n",
        "spam_train_documents = []\n",
        "ham_train_documents = []\n",
        "for i in range(len(train_files)):\n",
        "    if train_labels[i] == 1:  # for \"SPAM\" files\n",
        "        spam_train_document = read_file(spam_train_dir + train_files[i])\n",
        "        # candidate_features = getTokens(train_text)\n",
        "        spam_train_documents.append(spam_train_document)\n",
        "    elif train_labels[i] == 0:  # for \"HAM\" files\n",
        "        ham_train_document = read_file(ham_train_dir + train_files[i])\n",
        "        # candidate_features = getTokens(train_text)\n",
        "        ham_train_documents.append(ham_train_document)\n",
        "print('DONE\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2HmMd1dEnzt",
        "outputId": "752f62fd-3390-4881-c3d2-a75f853157d9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyCJpy9DEVJV",
        "outputId": "3dfb2b1f-7269-4e28-8e9c-cc7a1f424908"
      },
      "outputs": [],
      "source": [
        "print(\"Calculating feature token frequencies in SPAM files...\")\n",
        "token_frequencies_in_spam_class, spam_distinct_words, total_words_in_spam_class = \\\n",
        "    calculate_token_frequencies_in_class(feature_tokens, stop_words, spam_train_documents)\n",
        "print('DONE\\n')\n",
        "\n",
        "print(\"Calculating feature token frequencies in HAM files...\")\n",
        "token_frequencies_in_ham_class, ham_distinct_words, total_words_in_ham_class = \\\n",
        "    calculate_token_frequencies_in_class(feature_tokens, stop_words, ham_train_documents)\n",
        "print('DONE\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_aJ_mP6EtCu",
        "outputId": "30bf811c-c98c-460f-b35f-fe6fb8d723e8"
      },
      "outputs": [],
      "source": [
        "V = len(spam_distinct_words.union(ham_distinct_words))\n",
        "\n",
        "print('total words in spam class:', total_words_in_spam_class)\n",
        "print('total words in ham class:', total_words_in_ham_class)\n",
        "print('vocabulary size |V|:', V)\n",
        "print('')\n",
        "\n",
        "wrong_counter = 0  # the number of wrong classifications made by Logistic Regression\n",
        "\n",
        "true_positives = 0\n",
        "false_positives = 0\n",
        "true_negatives = 0\n",
        "false_negatives = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObFx0uwLE4sO",
        "outputId": "5a33ee92-7147-4764-9700-bbfde9bf1691"
      },
      "outputs": [],
      "source": [
        "# testing files with Naive Bayes classifier using Laplace estimates\n",
        "print(\"Reading TEST files...\")\n",
        "for i in range(len(test_files)):  # for all the test files that exist\n",
        "    test_text = ''\n",
        "    if test_true_labels[i] == 1:  # 1 is for class \"SPAM\"\n",
        "        test_text = read_file(spam_test_dir + test_files[i])\n",
        "    if test_true_labels[i] == 0:  # 0 is for class \"HAM\"\n",
        "        test_text = read_file(ham_test_dir + test_files[i])\n",
        "    test_text_tokens = word_tokenize(test_text)\n",
        "    filtered_test_text_tokens = [w.lower() for w in test_text_tokens if not w.lower() in stop_words]\n",
        "\n",
        "    test_feature_vector = [0] * len(feature_tokens)\n",
        "    for j in range(len(feature_tokens)):\n",
        "        if feature_tokens[j] in test_text_tokens:\n",
        "            test_feature_vector[j] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rki05A0-FAvt"
      },
      "outputs": [],
      "source": [
        "spam_laplace_estimate_probability = calculate_laplace_estimate_probability(\n",
        "    test_feature_vector,\n",
        "    feature_tokens,\n",
        "    spam_class_probability,\n",
        "    token_frequencies_in_spam_class,\n",
        "    total_words_in_spam_class,\n",
        "    V\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SDanhGOFMu0"
      },
      "outputs": [],
      "source": [
        " ham_laplace_estimate_probability = calculate_laplace_estimate_probability(\n",
        "            test_feature_vector,\n",
        "            feature_tokens,\n",
        "            ham_class_probability,\n",
        "            token_frequencies_in_ham_class,\n",
        "            total_words_in_ham_class,\n",
        "            V\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cgo6mzpFP4M",
        "outputId": "e02c6573-7106-4a33-a430-b9542e7ec594"
      },
      "outputs": [],
      "source": [
        "if spam_laplace_estimate_probability >= ham_laplace_estimate_probability and test_true_labels[i] == 1:\n",
        "    print(\"'\" + test_files[i] + \"'\" + \" classified as: SPAM -> correct\")\n",
        "    true_positives += 1\n",
        "elif spam_laplace_estimate_probability >= ham_laplace_estimate_probability and test_true_labels[i] == 0:\n",
        "    print(\"'\" + test_files[i] + \"'\" + \" classified as: SPAM -> WRONG!\")\n",
        "    wrong_counter += 1\n",
        "    false_positives += 1\n",
        "elif spam_laplace_estimate_probability < ham_laplace_estimate_probability and test_true_labels[i] == 0:\n",
        "    print(\"'\" + test_files[i] + \"'\" + \" classified as: HAM -> correct\")\n",
        "    true_negatives += 1\n",
        "elif spam_laplace_estimate_probability < ham_laplace_estimate_probability and test_true_labels[i] == 1:\n",
        "    print(\"'\" + test_files[i] + \"'\" + \" classified as: HAM -> WRONG!\")\n",
        "    wrong_counter += 1\n",
        "    false_negatives += 1\n",
        "\n",
        "print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFhg7-VwFslq",
        "outputId": "176b3666-417e-4ad0-f96e-8721c04a73b8"
      },
      "outputs": [],
      "source": [
        "print('Manual Naive-Bayes Classifier:')\n",
        "print('Number of features used: ' + str(len(feature_tokens)))\n",
        "print('')\n",
        "\n",
        "# Accuracy\n",
        "accuracy = ((len(test_files) - wrong_counter) / len(test_files)) * 100\n",
        "print(\"Accuracy: \" + str(accuracy) + \" %\")\n",
        "print('')\n",
        "\n",
        "\n",
        "# Total duration\n",
        "print(\"Total duration: %s seconds\" % (time.time() - start_time))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
